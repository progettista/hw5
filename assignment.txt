1.1: Balanced corpus of at least 20 utterances.
The corpus is balanced if half of it is in-domain, and half is out-of-domain.
Make sure the corpus is rich enough, namely, it's not too trivial to come up with rules that separate in-domain from out-of-domain.
Any corpus that is representative enough of real conversation will be rich.

1.2: Simple FSM that covers some of the in-domain utterances. 

Deliverable: FSM and code.

1.3: Run (by talking to your chatbot or by analyzing the FSM) your corpus through your chatbot.
Mark the performance on every entry of the corpus.

Deliverable: Table with 3 colimns: Chatbot performance (+ for correct, - for incorrect), ground truth (+ for in-domain utterance, - for out-of-domain utterance), and the utterance itself.

1.4: Compute TP, FP, FN, TN, Recall and Precision measures for your chatbot.

If your corpus is rich enough, both FP and FN should be non-zero.
If they are zero, enrich your corpus with more complex examples to make it more realistic.

Repeat the procedure in attempt to reduce both FP and FN rates.

1.5: Go trough this cycle 3 times.
In the end you will have a corpus and 3 different instances of each:

FSM+code

Table of performance of the chatbot on the corpus and the performance metrics (TP, FP, FN, TN, Recall, Precision)

1.6: Plot the 3 values of Recall and Precision for each of the chatbot versions on the ROC curve.

Discuss which one would you pick as your final chatbot and why.

1.7: Deliver all the deliverables by commiting it to your personal GitHub account (put it in a folder called "hw5").
Your Google classroom submission should consist of only your account's URL.
